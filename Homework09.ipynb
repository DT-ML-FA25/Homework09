{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QshK8s21WBrf"
   },
   "source": [
    "# Homework09\n",
    "\n",
    "Exercises to practice unsupervised learning with clustering\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Get more practice with the ML flow: encode -> normalize -> train -> evaluate\n",
    "- Understand the tradeoffs of modeling parameters\n",
    "- Develop intuition for different clustering models and when to use them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Hf8SXUwWOho"
   },
   "source": [
    "### Setup\n",
    "\n",
    "Run the following 2 cells to import all necessary libraries and helpers for this homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://github.com/PSAM-5020-2025F-A/5020-utils/raw/main/src/data_utils.py\n",
    "!wget -q https://github.com/PSAM-5020-2025F-A/5020-utils/raw/main/src/image_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from os import listdir\n",
    "from PIL import Image as PImage\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from data_utils import balance_score, distance_score, silhouette_score, display_silhouette_plots\n",
    "from data_utils import object_from_json_url\n",
    "\n",
    "from image_utils import get_pixels, make_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helmet Sizing\n",
    "\n",
    "### Load Dataset\n",
    "\n",
    "Let's load up the full [ANSUR](https://www.openlab.psu.edu/ansur2/) dataset that we looked at briefly in [Week 02](https://github.com/PSAM-5020-2025F-A/WK02) and then again in [Homework07](https://github.com/PSAM-5020-2025F-A/Homework07).\n",
    "\n",
    "This is the dataset that has anthropometric information about U.S. Army personnel.\n",
    "\n",
    "#### WARNING\n",
    "\n",
    "Like we mentioned in class, this dataset is being used for these exercises due to the level of detail in the dataset and the rigorous process that was used in collecting the data.\n",
    "\n",
    "This is a very specific dataset and should not be used to draw general conclusions about people, bodies, or anything else that is not related to the distribution of physical features of U.S. Army personnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "ANSUR_FILE = \"https://raw.githubusercontent.com/PSAM-5020-2025F-A/5020-utils/main/datasets/json/ansur.json\"\n",
    "ansur_data = object_from_json_url(ANSUR_FILE)\n",
    "\n",
    "# Look at first 2 records\n",
    "ansur_data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load it into a `DataFrame`, like last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read into DataFrame\n",
    "ansur_df = pd.json_normalize(ansur_data)\n",
    "ansur_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "Let's pretend we are designing next-generation helmets with embedded over-the-ear headphones and we want to have a few options for sizes.\n",
    "\n",
    "We could use clustering to see if there is a number of clusters that we can divide our population into, so each size covers a similar portion of the population.\n",
    "\n",
    "We can follow similar steps to regression to create a clustering model that uses features about head and ear sizes:\n",
    "\n",
    "1. Load dataset (done! ðŸŽ‰)\n",
    "2. Encode label features as numbers\n",
    "3. Normalize the data\n",
    "4. Separate the feature variables we want to consider (done below)\n",
    "5. Pick a clustering algorithm\n",
    "6. Determine number of clusters\n",
    "7. Cluster data\n",
    "8. Interpret results\n",
    "\n",
    "For step $5$, it's fine to just pick an algorithm ahead of time to see what happens, but feel free to experiment and plot results for multiple clustering methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "## Encode non-numerical features\n",
    "encoder = OrdinalEncoder()\n",
    "ansur_encoded_df = ansur_df.copy()\n",
    "\n",
    "# Encode all object (non-numeric) columns\n",
    "for col in ansur_encoded_df.select_dtypes(include=\"object\").columns:\n",
    "    ansur_encoded_df[col] = encoder.fit_transform(ansur_encoded_df[[col]])\n",
    "\n",
    "\n",
    "## Normalize the data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "ansur_scaled_df = ansur_encoded_df.copy()\n",
    "ansur_scaled_df[ansur_scaled_df.columns] = scaler.fit_transform(ansur_encoded_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate the features we want to consider\n",
    "ansur_features = ansur_scaled_df[[\"head.height\", \"head.circumference\", \"ear.length\", \"ear.breadth\", \"ear.protrusion\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "## Create Clustering model\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "## Run the model(s) on the data\n",
    "kmeans_labels = kmeans.fit_predict(ansur_features)\n",
    "ansur_features[\"cluster\"] = kmeans_labels\n",
    "\n",
    "## Check errors\n",
    "print(\"Silhouette Score:\", silhouette_score(ansur_features.drop(\"cluster\", axis=1), kmeans_labels))\n",
    "print(\"Balance Score:\", balance_score(kmeans_labels))\n",
    "print(\"Distance Score:\", distance_score(ansur_features.drop(\"cluster\", axis=1), kmeans_labels))\n",
    "\n",
    "\n",
    "## Plot clusters as function of 2 or 3 variables\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(ansur_features[\"head.circumference\"], ansur_features[\"head.height\"], c=ansur_features[\"cluster\"], cmap=\"viridis\")\n",
    "plt.xlabel(\"Head Circumference\")\n",
    "plt.ylabel(\"Head Height\")\n",
    "plt.title(\"Helmet Size Clusters\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "Which clustering algorithm did you choose?<br>\n",
    "Did you try a different one?<br>\n",
    "Do the clusters make sense ? Do they look balanced ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">\n",
    "I used the K-Means algorithm to group the data into three clusters. I also tried using Gaussian Mixture, which gave a similar result but with smoother boundaries between groups.\n",
    "\n",
    "The clusters look quite reasonable, they seem to group people by overall head size, from smaller to larger heads. The distribution looks fairly balanced, which makes sense for designing helmet sizes like small, medium, and large. Thereâ€™s some overlap between the clusters, but that makes sense as real human measurements usually blend gradually too rather than forming clear separations.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure out how many cluster\n",
    "\n",
    "Experiment with the number of clusters to see if the initial choice makes sense.\n",
    "\n",
    "The [WK09](https://github.com/PSAM-5020-2025F-A/WK09) notebook had a for loop that can be used to plot errors versus number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "## Plot errors and pick how many cluster\n",
    "sil_scores = []\n",
    "dist_scores = []\n",
    "cluster_range = range(2, 10)\n",
    "\n",
    "for k in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(ansur_features.drop(\"cluster\", axis=1, errors=\"ignore\"))\n",
    "    sil = silhouette_score(ansur_features.drop(\"cluster\", axis=1, errors=\"ignore\"), labels)\n",
    "    dist = distance_score(ansur_features.drop(\"cluster\", axis=1, errors=\"ignore\"), labels)\n",
    "    \n",
    "    sil_scores.append(sil)\n",
    "    dist_scores.append(dist)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(cluster_range, sil_scores, marker='o', label=\"Silhouette Score\")\n",
    "plt.plot(cluster_range, dist_scores, marker='x', label=\"Distance Score\")\n",
    "plt.xlabel(\"Number of Clusters\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Choosing the Best Number of Clusters\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "Based on the graphs of errors versus number of clusters, does it look like we should change the initial number of clusters ?<br>\n",
    "How many clusters should we use ? Why ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">Looking at the graphs, the scores get better until around 3 clusters, and after that they donâ€™t change much. This means using more clusters doesnâ€™t really improve the results. So, 3 clusters seems like the best choice because it keeps things simple and still separates the data well.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revise Number of Clusters.\n",
    "\n",
    "Re-run with the new number of clusters and plot the data in $2D$ or $3D$.\n",
    "\n",
    "This can be the same graph as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Re-run clustering with final number of clusters\n",
    "FINAL_K = 3\n",
    "kmeans_final = KMeans(n_clusters=FINAL_K, random_state=42)\n",
    "\n",
    "# Run the model on the training data\n",
    "final_labels = kmeans_final.fit_predict(ansur_features.drop(\"cluster\", axis=1, errors=\"ignore\"))\n",
    "ansur_features[\"cluster\"] = final_labels\n",
    "\n",
    "# Check errors\n",
    "print(\"Silhouette Score:\", silhouette_score(ansur_features.drop(\"cluster\", axis=1), final_labels))\n",
    "print(\"Balance Score:\", balance_score(final_labels))\n",
    "print(\"Distance Score:\", distance_score(ansur_features.drop(\"cluster\", axis=1), final_labels))\n",
    "\n",
    "# Plot in 3D\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(\n",
    "    ansur_features[\"head.circumference\"], \n",
    "    ansur_features[\"head.height\"], \n",
    "    ansur_features[\"ear.length\"], \n",
    "    c=ansur_features[\"cluster\"], cmap=\"viridis\"\n",
    ")\n",
    "ax.set_xlabel(\"Head Circumference\")\n",
    "ax.set_ylabel(\"Head Height\")\n",
    "ax.set_zlabel(\"Ear Length\")\n",
    "ax.set_title(\"3D Cluster Visualization (K=3)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "Do these look better than the original number of clusters?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">Yes, these clusters look better than before. With three clusters, the data is grouped more clearly, and the points form distinct regions in the 3D plot. The clusters also seem more balanced in size and make more sense for dividing helmet sizes into small, medium, and large. Adding more clusters didnâ€™t really improve the separation, so three works well for this dataset.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Organization\n",
    "\n",
    "We have a dataset of about $600$ flower images that we might want to classify by species... eventually.\n",
    "\n",
    "What we want to do first is take a look at all of the images and see what kind of images we have, what kind of colors our flowers have and see if there's any other visual information that could help us classify these images later.\n",
    "\n",
    "We'll see how to use clustering and distances to organize our images by color to create a visualization that we cna use to get to know our dataset.\n",
    "\n",
    "### Load Dataset\n",
    "\n",
    "The following cell downloads the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -qO- https://github.com/PSAM-5020-2025F-A/5020-utils/releases/latest/download/flowers.tar.gz | tar xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can take a look at a few of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DIR = \"./data/image/flowers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(PImage.open(f\"{IMG_DIR}/00_001.png\"))\n",
    "display(PImage.open(f\"{IMG_DIR}/15_001.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Representative Colors\n",
    "\n",
    "The overall process for organizing our images by color will be something like this:\n",
    "\n",
    "1. Iterate over all files in the `data/image/flowers` directory, open each image file and treat it as a dataset\n",
    "   1. Load image into a `DataFrame` where each pixel is a row and R,G,B values are columns/features\n",
    "   2. Cluster into $2$ - $16$ colors\n",
    "   3. Pick $3$ or $4$ representative colors\n",
    "   4. Store image filenames and their representative colors in a Python object\n",
    "2. Once all images have been processed we can order our dataset by different color characteristics: white to black, red to blue, hue value, brightness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Image\n",
    "\n",
    "Let's step through the process of getting representative colors for one image, and then we can repeat this in a loop to process all of the flower images.\n",
    "\n",
    "#### Open Image\n",
    "\n",
    "The `PIL` library does all the work here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open image\n",
    "fname = \"00_001.png\"\n",
    "pimg = PImage.open(f\"{IMG_DIR}/{fname}\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put into `DataFrame`\n",
    "\n",
    "We get the pixels and make a dataset/`DataFrame` out of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into DataFrame\n",
    "pxs = get_pixels(pimg)\n",
    "pxs_df = pd.DataFrame(pxs, columns=[\"R\", \"G\", \"B\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster colors\n",
    "\n",
    "Create a clustering object, cluster colors into $8$ clusters with `fit_predict()` and take a look at our color palette (`cluster_centers_`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: Create Clustering object\n",
    "kmeans_colors = KMeans(n_clusters=5, random_state=42)\n",
    "\n",
    "# TODO: Cluster by color\n",
    "kmeans_colors.fit(pxs_df)\n",
    "\n",
    "# TODO: Take a look at the color palette (cluster_centers_)\n",
    "print(kmeans_colors.cluster_centers_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoint\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "Does anything stand out about the colors?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">The colors match the main tones of the flower image, showing a good mix of bright and dark shades that represent the petals and background.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reconstruct Image\n",
    "\n",
    "Since we're only doing one image for now, let's take a look at the clustering result.\n",
    "\n",
    "This is like in the lecture notebook. We'll start with an empty pixel array and as we iterate through the `DataFrame` of cluster ids we append the corresponding colors to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: create empty pixel array\n",
    "pxs_post = []\n",
    "\n",
    "# TODO: iterate through resulting list of cluster ids\n",
    "for cid in kmeans_colors.labels_:\n",
    "    # TODO: append corresponding color value, converting to int\n",
    "    pxs_post.append([int(c) for c in kmeans_colors.cluster_centers_[cid]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at the image. If this next cell gives errors about using `float` values in images, just make sure the pixel values that are being appended above are all whole number `int` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(make_image(pxs_post, width=pimg.size[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoint\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "How does changing the number of clusters affect the resulting image?<br>Try some lower values like <code>2</code> and <code>4</code>, and also some higher ones like <code>12</code> and <code>16</code>. Take a look at a different image.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">Fewer clusters (like 2-4) make the image very simplified with only a few colors, while more clusters (like 12-16) capture more detail and look closer to the original. Moderate numbers (around 5-8) give a good balance.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pick Colors\n",
    "\n",
    "Ok, we have some representative colors for our images. We should keep more than one color, but maybe we don't have to keep $12$.\n",
    "\n",
    "If we put our predictions in a `DataFrame` we can use the `value_counts()` function of our `DataFrame` to see how many pixels are represented by each of our cluster colors, and get the result ordered by most frequent to less frequent cluster id:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster ids and pixel counts, ordered by descending counts\n",
    "px_clusters_df = pd.DataFrame(kmeans_colors.labels_, columns=[\"clusters\"])\n",
    "ccounts = px_clusters_df[\"clusters\"].value_counts()\n",
    "display(ccounts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since what we are really trying to do here is get some information about the colors of the flowers present in our images, and given the type of images we have, we can start by assuming that the flower colors will be in the top-$4$ clusters returned by `value_counts()`.\n",
    "\n",
    "We can revisit this assumption later. We might also want to add some filters here to ignore sky and vegetation colors (blues and greens) and only keep flower colors.\n",
    "\n",
    "For now, let's just grab the top $4$ colors from `value_counts()`, remembering we want to keep their rounded `int` values and not the default `float` values in `cluster_centers_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Object to keep colors for each file\n",
    "file_info = {\n",
    "  \"filename\": fname,\n",
    "  \"colors\": []\n",
    "}\n",
    "\n",
    "# TODO: go through ccounts.index and get corresponding colors for each clusters\n",
    "\n",
    "# TODO: add top-4 colors to the \"colors\" key of the file_color_info object\n",
    "for cluster_id in ccounts.index[:4]:  # top 4 clusters\n",
    "    # Get the cluster color and convert to int\n",
    "    color = [int(c) for c in kmeans_colors.cluster_centers_[cluster_id]]\n",
    "    file_info[\"colors\"].append(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(file_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoint\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "Why might we want to cluster into <code>8</code> or even <code>12</code> colors when in the end we're only keeping <code>4</code>?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">Clustering into more colors (like 8 or 12) helps capture subtle variations in the image. Even if we only keep the top 4 most frequent colors, having more clusters ensures that the dominant colors we choose are more accurate and not averaged with minor shades.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate and Cluster\n",
    "\n",
    "We've processed one image, now let's process $600$... for-loops FTW!\n",
    "\n",
    "We'll need to loop through all of the images in our directory and repeat the process above for each one of them.\n",
    "\n",
    "We can create a function that takes a filename as input and returns the top-$4$ colors for that image, or... we can just put all of the clustering logic in the body of a for loop. Whichever is easiest.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all files in the flowers directory\n",
    "flower_files = sorted([f for f in listdir(IMG_DIR) if f.endswith(\".png\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the loop. In the end we want our `file_colors` list to have objects that have a filename and $4$ colors associated with each filename. Something like:\n",
    "\n",
    "```py\n",
    "[\n",
    "  {\n",
    "    \"filename\": \"00_001.png\",\n",
    "    \"colors\": [[12,44,12], [112,144,62],  [12,84,112], [212,144,102]]\n",
    "  },\n",
    "  {\n",
    "    \"filename\": \"00_002.png\",\n",
    "    \"colors\": [[22,24,28], [112,114,122], [128,200,2], [250,240,230]]\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "This can take a while to run (up to a minute for $600$ images). We can use slicing to test our logic on a subset of `flower_files` before processing all $600$ images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to keep colors for each file\n",
    "file_colors = []\n",
    "\n",
    "# TODO: get colors for each image\n",
    "for fname in flower_files:\n",
    "    # TODO: add logic here\n",
    "    # Open image and convert to RGB\n",
    "    pimg = PImage.open(f\"{IMG_DIR}/{fname}\").convert(\"RGB\")\n",
    "    \n",
    "    # Get pixels as a DataFrame\n",
    "    pxs = get_pixels(pimg)\n",
    "    pxs_df = pd.DataFrame(pxs, columns=[\"R\", \"G\", \"B\"])\n",
    "    \n",
    "    # Cluster pixels into 8 colors (can adjust this number)\n",
    "    kmeans_colors = KMeans(n_clusters=8, random_state=42)\n",
    "    kmeans_colors.fit(pxs_df)\n",
    "    \n",
    "    # Count pixels per cluster\n",
    "    px_clusters_df = pd.DataFrame(kmeans_colors.labels_, columns=[\"clusters\"])\n",
    "    ccounts = px_clusters_df[\"clusters\"].value_counts()\n",
    "    \n",
    "    # TODO: add filename+colors object to list of objects\n",
    "    # Pick top 4 colors\n",
    "    top_colors = []\n",
    "    for cluster_id in ccounts.index[:4]:\n",
    "        color = [int(c) for c in kmeans_colors.cluster_centers_[cluster_id]]\n",
    "        top_colors.append(color)\n",
    "    \n",
    "    file_colors.append({\n",
    "        \"filename\": fname,\n",
    "        \"colors\": top_colors\n",
    "    })\n",
    "\n",
    "# Display first few results to check\n",
    "file_colors[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order Images (almost)\n",
    "\n",
    "We have a list with objects that keep track of filenames and representative colors. We could create a `DataFrame` or csv dataset with these, but let's go ahead and just use this directly in this format.\n",
    "\n",
    "What we want to do is re-order our list of objects, but using a `key` function that takes each object's colors into consideration.\n",
    "\n",
    "We'll look into how to do this dynamically later, but for now let's order our images by something like _brightness_. It's _like_ brightness because what we'll do is measure how close each image is to the white color `(255,255,255)`.\n",
    "\n",
    "We'll need some helper functions first:\n",
    "\n",
    "- `color_distance()`: takes $2$ colors and returns the distance between them\n",
    "- `min_color_distance()`: given a reference color and a list of colors, returns the distance between the reference color and the closest color in the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement function that returns distance between two colors\n",
    "def color_distance(c0, c1):\n",
    "    # TODO: add logic here\n",
    "    return math.sqrt((c0[0]-c1[0])**2 + (c0[1]-c1[1])**2 + (c0[2]-c1[2])**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tests for the `color_distance()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tests for the color_distance() function\n",
    "print(color_distance([0,0,0], [255,255,255]), \"should be\", 255*3**.5)\n",
    "print(color_distance([0,100,0], [100,100,0]), \"should be\", 100)\n",
    "print(color_distance([55,222,120], [91,51,192]), \"should be\", 189)\n",
    "print(color_distance([147,207,246], [87,57,50]), \"should be\", 254)\n",
    "print(color_distance([12,250,126], [112,10,195]), \"should be\", 269)\n",
    "print(color_distance([106,71,61], [105,136,100]), \"should be\", 75.81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: implement function that returns minimum distance between a reference color and colors from a list\n",
    "def min_color_distance(ref_color, color_list):\n",
    "  # TODO: add logic here\n",
    "  distances = [color_distance(ref_color, c) for c in color_list]\n",
    "  return min(distances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three tests for the `min_color_distance()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tests for the color_distance() function\n",
    "print(min_color_distance([0,0,0], [[255,255,255],[0,100,0],[100,100,0],[58,58,58]]), \"should be\", 100)\n",
    "print(min_color_distance([0,0,0], [[255,255,255],[0,100,0],[100,100,0],[58,57,58]]), \"should be\", 99.88)\n",
    "print(min_color_distance([91,51,192], [[147,207,246],[87,57,50],[12,250,126],[112,10,195]]), \"should be\", 46.16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Order Images (for real now)\n",
    "\n",
    "Alright. We have a function that can be used to order our images by their distance to a given color.\n",
    "\n",
    "Let's order our images by how close they are to the brightest color `(255,255,255)`. We'll define a `key` function that, given an object from our `file_colors` list, returns how close that image is to the color `(255,255,255)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: implement function that returns how close our image is to the color white\n",
    "def by_bright_dist(A):\n",
    "  # TODO: add logic here\n",
    "  colors = A[\"colors\"]\n",
    "  return min_color_distance([255, 255, 255], colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Order the list and write out a `JSON` file with the image order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_colors_sorted = sorted(file_colors, key=by_bright_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_sorted = [A[\"filename\"] for A in file_colors_sorted]\n",
    "\n",
    "with open(\"./data/flower_order.json\", \"w\") as ofp:\n",
    "  json.dump(files_sorted, ofp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Results\n",
    "\n",
    "We can check the results by running a webserver and looking at a simple web page that orders the images according to the resulting `JSON` file from above.\n",
    "\n",
    "We'll make use of the [`Live Server`](https://marketplace.visualstudio.com/items?itemName=ritwickdey.LiveServer) VSCode extension.\n",
    "\n",
    "We can start the server by clicking on the \"_Go Live_\" button towards the right hand side of the bar at the very bottom of our text editor:\n",
    "\n",
    "<img src=\"./imgs/go_live.jpg\" width=\"600px\">\n",
    "\n",
    "Clicking the \"_Go Live_\" button in Codespace should open up a new tab with a plain html navigation view of our repository. Clicking on the `html/` directory should open up a web page with all of the flower images. If not, you can use your Codespace url to try to find the web server address.\n",
    "\n",
    "If your Codespace url is something like:<br>`https://mango-special-giggle-v6v7asd322f7p6.github.dev/`\n",
    "\n",
    "Then, the webserver should be running at:<br>`https://mango-special-giggle-v6v7asd322f7p6-5500.app.github.dev/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review, Contemplate, Experiment\n",
    "\n",
    "Yes, images with white parts are towards the beginning, but the images towards the end aren't necessarily the ones with dark flowers, but are the ones that have all of their representative colors farthest away from white `(255,255,255)`, which includes very saturated colors/images.\n",
    "\n",
    "A couple of interesting experiments here could be:\n",
    "- Decrease the number of clusters or the number of colors kept after clustering.\n",
    "- Use different colors as the reference for the distance functions. For example, create `by_gold_dist()` or `by_purple_dist()` functions to use as the `key` for sorting.\n",
    "- Order the list of cluster colors by [hue](https://stackoverflow.com/questions/23090019/fastest-formula-to-get-hue-from-rgb). This can be a bit tricky to get right because some colors, like white, black and gray, don't have a unique value for hue, but depend on other aspects of the color, like saturation and lightness, to be well-defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: experiment with number of clusters, number of colors, reference colors or hue distances\n",
    "# Re-cluster all images with 12 clusters and keep top 4 colors\n",
    "file_colors_all = []\n",
    "\n",
    "for fname in flower_files:  # remove the [:5] slicing to include all images\n",
    "    pimg = PImage.open(f\"{IMG_DIR}/{fname}\").convert(\"RGB\")\n",
    "    pxs_df = pd.DataFrame(get_pixels(pimg), columns=[\"R\", \"G\", \"B\"])\n",
    "    \n",
    "    # Cluster colors and keep top 4\n",
    "    kmeans_colors = KMeans(n_clusters=12, random_state=42)\n",
    "    kmeans_colors.fit(pxs_df)\n",
    "    px_clusters_df = pd.DataFrame(kmeans_colors.labels_, columns=[\"clusters\"])\n",
    "    ccounts = px_clusters_df[\"clusters\"].value_counts()\n",
    "    top_colors = [[int(c) for c in kmeans_colors.cluster_centers_[i]] for i in ccounts.index[:4]]\n",
    "    \n",
    "    file_colors_all.append({\"filename\": fname, \"colors\": top_colors})\n",
    "\n",
    "# Sort all images by gold color\n",
    "def by_gold_dist(A):\n",
    "    return min_color_distance([255, 215, 0], A[\"colors\"])\n",
    "\n",
    "file_colors_sorted = sorted(file_colors_all, key=by_gold_dist)\n",
    "\n",
    "# Check the sorted filenames\n",
    "files_sorted = [f[\"filename\"] for f in file_colors_sorted]\n",
    "files_sorted\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "<span style=\"color:hotpink;\">\n",
    "What did you try ? What happened ?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "work_cell"
    ]
   },
   "source": [
    "<span style=\"color:hotpink;\">\n",
    "I re-clustered all the images using 12 clusters and kept the top 4 colors. Then I sorted them by how close their colors are to gold.\n",
    "\n",
    "* The images with more gold tones came first, and those with less gold came later.\n",
    "\n",
    "* Changing the reference color changes the order of the images.\n",
    "\n",
    "* Using more clusters helped capture subtle colors, so the sorting looks more accurate.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "It's challenging to define a set of functions that will perfectly order our flowers by color without first having to define very specific color values for filtering and corner-cases. At a high-level, we can imagine that this is because color is a $3$-dimensional value, and we're using it to organize our images into a single-dimensional order.\n",
    "\n",
    "The beginning of our ordering is usually pretty good, since there's only one way for a color to be _close_ to our reference color, but the ordering gets less consistent towards the end because there are many different ways for a color to be _far_ from the reference color.\n",
    "\n",
    "Next week we'll see a very powerful technique that, amongst other things, will help us get around this kind of \"_dimensionality mismatch_\"."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
